import warnings
warnings.filterwarnings("ignore")

import argparse
import logging
from langchain_community.llms import LlamaCpp
from langchain.prompts import PromptTemplate
from langchain_core.callbacks import CallbackManager, \
StreamingStdOutCallbackHandler

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class LLModel:
    """
    A class to interact with the LlamaCpp model.

    This class initializes the model with the specified prompt and model path,
    allowing you to generate responses based on the given prompt.

    Attributes:
        model_path (str): Path to the model file.
        prompt (str): The prompt to be processed.
        llama_chain: A chain of prompt and model for generating responses.
    """
    
    def __init__(self, path: str = 'model/1B/llama-3.2-1b-instruct-q4_k_m.gguf'):
        """
        Initializes the LLModel with a prompt and model path.

        Args:
            prompt (str): The prompt to generate a response for.
            path (str): Path to the model file. Defaults to 'model/1B/llama-3.2-1b-instruct-q4_k_m.gguf'.
        """
        self.model_path = path
        callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])
        # Initialize the LlamaCpp model
        self.llm = LlamaCpp(
            model_path=self.model_path,
            n_gpu_layers=1,
            n_batch=512,
            temperature=0.75,
            f16_kv=True,            
            n_threads=8,            # Number of CPU threads to use
            callback_manager=callback_manager,
            verbose=False
        )
        self.create_prompt()
        self.llama_chain = self.prompt_template | self.llm

    
    def create_prompt(self) -> str:
        self.prompt_template = PromptTemplate(
            input_variables=["system_prompt", "user_message"],
            template="""
                <|begin_of_text|>
                    <|start_header_id|>system<|end_header_id|>
                        {system_prompt}<|eot_id|>
                    <|start_header_id|>user<|end_header_id|>
                        {user_message}<|eot_id|>
                    <|start_header_id|>assistant<|end_header_id|>
                <|end_of_text|>
                """
        )

    def run(self, s_prompt: str, u_mesg: str) -> str:
        """
        Executes the model with the specified prompt and returns the generated response.

        Returns:
            str: The response generated by the model.
        """
        response = self.llama_chain.invoke({"system_prompt": s_prompt, "user_message": u_mesg})
        return response



if __name__ == '__main__':
    # Create the argument parser
    parser = argparse.ArgumentParser(description="Interact with LlamaCpp model.")
    
    # Add arguments
    parser.add_argument("--model", type=str, required=True, help="Path to the model file.")
    parser.add_argument("--prompt", type=str, required=True, help="The prompt to generate a response for.")
    parser.add_argument("--verbose", action="store_true", help="Enable verbose output")

    # Parse the arguments
    args = parser.parse_args()
    
    # Initialize and run the model
    model = LLModel(args.model)
    system_prompt = "Please respond to the following question in no more than 200 words. \
                    You are a helpful AI that responds in markdown format."
    response = model.run(s_prompt=system_prompt, u_mesg=args.prompt)
    
    # Print the response
    if args.verbose:
        print("Verbose Output:")
        print(response)