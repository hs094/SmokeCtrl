{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e582ca4e",
   "metadata": {
    "papermill": {
     "duration": 0.004112,
     "end_time": "2025-01-18T13:06:05.611581",
     "exception": false,
     "start_time": "2025-01-18T13:06:05.607469",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Llama 3.2 7B Model Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a396d41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T13:06:05.619780Z",
     "iopub.status.busy": "2025-01-18T13:06:05.619463Z",
     "iopub.status.idle": "2025-01-18T13:06:49.401631Z",
     "shell.execute_reply": "2025-01-18T13:06:49.400716Z"
    },
    "papermill": {
     "duration": 43.788185,
     "end_time": "2025-01-18T13:06:49.403365",
     "exception": false,
     "start_time": "2025-01-18T13:06:05.615180",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/3.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m120.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m60.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/9.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/9.7 MB\u001b[0m \u001b[31m199.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m195.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m106.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/69.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/69.1 MB\u001b[0m \u001b[31m109.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.3/69.1 MB\u001b[0m \u001b[31m157.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.0/69.1 MB\u001b[0m \u001b[31m189.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.5/69.1 MB\u001b[0m \u001b[31m186.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.1/69.1 MB\u001b[0m \u001b[31m189.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.9/69.1 MB\u001b[0m \u001b[31m191.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/69.1 MB\u001b[0m \u001b[31m194.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m50.4/69.1 MB\u001b[0m \u001b[31m197.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m57.4/69.1 MB\u001b[0m \u001b[31m196.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m63.8/69.1 MB\u001b[0m \u001b[31m192.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m197.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m197.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m197.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m197.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m197.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m197.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m197.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m197.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m197.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m197.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/293.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.4/293.4 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/374.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.8/374.8 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/450.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m450.7/450.7 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/3.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m97.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -q huggingface_hub\n",
    "!pip install -Uq transformers[torch] datasets\n",
    "!pip install -q bitsandbytes trl peft accelerate\n",
    "!pip install -q flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f8dae92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T13:06:49.414157Z",
     "iopub.status.busy": "2025-01-18T13:06:49.413905Z",
     "iopub.status.idle": "2025-01-18T13:06:49.417519Z",
     "shell.execute_reply": "2025-01-18T13:06:49.416836Z"
    },
    "papermill": {
     "duration": 0.010354,
     "end_time": "2025-01-18T13:06:49.418839",
     "exception": false,
     "start_time": "2025-01-18T13:06:49.408485",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_path = '/kaggle/input/medical-intelligence-dataset-40k-disease-info-qa'\n",
    "model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "hf_token = 'hf_api_token'\n",
    "wandb_token = 'wandb_api_key'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbc7c920",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T13:06:49.429072Z",
     "iopub.status.busy": "2025-01-18T13:06:49.428863Z",
     "iopub.status.idle": "2025-01-18T13:07:07.173333Z",
     "shell.execute_reply": "2025-01-18T13:07:07.172672Z"
    },
    "papermill": {
     "duration": 17.751162,
     "end_time": "2025-01-18T13:07:07.174705",
     "exception": false,
     "start_time": "2025-01-18T13:06:49.423543",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "848fdd5a442542669f1e0e33f2816b62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "import warnings\n",
    "import random\n",
    "warnings.filterwarnings('ignore')\n",
    "from datasets import Dataset, load_dataset, DatasetDict\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from time import time\n",
    "from datasets import load_dataset\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from trl import SFTTrainer,setup_chat_format\n",
    "from peft import LoraConfig\n",
    "import wandb\n",
    "\n",
    "user_secrets = UserSecretsClient()\n",
    "HF_TOKEN = user_secrets.get_secret(hf_token)\n",
    "wandb_key = user_secrets.get_secret(wandb_token)\n",
    "username = user_secrets.get_secret(\"hf_username\")\n",
    "repository_name = user_secrets.get_secret(\"hf_mtp_repo\")\n",
    "raw_datasets = load_dataset(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b938848a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T13:07:07.185877Z",
     "iopub.status.busy": "2025-01-18T13:07:07.185644Z",
     "iopub.status.idle": "2025-01-18T13:07:10.152966Z",
     "shell.execute_reply": "2025-01-18T13:07:10.151801Z"
    },
    "papermill": {
     "duration": 2.97469,
     "end_time": "2025-01-18T13:07:10.154776",
     "exception": false,
     "start_time": "2025-01-18T13:07:07.180086",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid (permission: fineGrained).\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token `kaggle_acsess_token` has been saved to /root/.cache/huggingface/stored_tokens\r\n",
      "Your token has been saved to /root/.cache/huggingface/token\r\n",
      "Login successful.\r\n",
      "The current active token is: `kaggle_acsess_token`\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\r\n"
     ]
    }
   ],
   "source": [
    "! huggingface-cli login --token $HF_TOKEN\n",
    "! wandb login $wandb_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "caf3e8f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T13:07:10.166600Z",
     "iopub.status.busy": "2025-01-18T13:07:10.166295Z",
     "iopub.status.idle": "2025-01-18T13:07:10.190060Z",
     "shell.execute_reply": "2025-01-18T13:07:10.189327Z"
    },
    "papermill": {
     "duration": 0.031032,
     "end_time": "2025-01-18T13:07:10.191383",
     "exception": false,
     "start_time": "2025-01-18T13:07:10.160351",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input', 'output'],\n",
       "        num_rows: 32353\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input', 'output'],\n",
       "        num_rows: 8089\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_split_ratio = 0.2\n",
    "\n",
    "# Split the dataset\n",
    "split_datasets = raw_datasets[\"train\"].train_test_split(test_size=0.2)\n",
    "\n",
    "# Wrap the splits into a DatasetDict\n",
    "raw_datasets = DatasetDict({\n",
    "    \"train\": split_datasets[\"train\"],\n",
    "    \"test\": split_datasets[\"test\"]\n",
    "})\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "446d8fb0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T13:07:10.202964Z",
     "iopub.status.busy": "2025-01-18T13:07:10.202739Z",
     "iopub.status.idle": "2025-01-18T13:07:10.216846Z",
     "shell.execute_reply": "2025-01-18T13:07:10.216155Z"
    },
    "papermill": {
     "duration": 0.021091,
     "end_time": "2025-01-18T13:07:10.218072",
     "exception": false,
     "start_time": "2025-01-18T13:07:10.196981",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': ['What is (are) Parasites - Echinococcosis ?',\n",
       "  'MY DAUGHTER IS 9 YRS OLD AND HAS A RED PATCH ,VERY SMALL, AROUND 2CM IN DIAMETER ON HER SCALP WHICH IS SCALY AND FLAKES OUT LEAVING REDDISH PATCH. DERMATOLOGISTS SUSPECT SEBORRHIC DERMATITIS/ PSORIASIS AS IT IS NOT GOING SINCE A YEAR NOW. ON APPLYING TRIBEN-B AND NOSKORF LOTION 2-3 TIMES A DAY IT DISAPPEARS WITHIN 10 DAYS BUT REAPPEARS ON STOPPING. kindly tell whether is it psoriasis and am i on the right path.'],\n",
       " 'output': ['Frequently Asked Questions (FAQs)\\n    \\nCystic echinococcosis (CE) disease results from being infected with the larval stage of Echinococcus granulosus, a tiny tapeworm (~2-7 millimeters in length) found in dogs (definitive host), sheep, cattle, goats, foxes, and pigs, amongst others (intermediate hosts). Most infections in humans are asymptomatic, but CE, also known as hydatid disease, causes slowly enlarging masses, most commonly in the liver and the lungs. Treatment can involve both medication and surgery.\\n    \\nMore on: Cystic Echinococcosis (CE) FAQs\\n    \\nAlveolar echinococcosis (AE) disease results from being infected with the larval stage of Echinococcus multilocularis, a tiny tapeworm (~1-4 millimeters in length) found in foxes, coyotes, dogs, and cats (definitive hosts). Although human cases are rare, infection in humans causes parasitic tumors to form in the liver, and, less commonly, the lungs, brain, and other organs. If left untreated, infection with AE can be fatal.\\n    \\nMore on: Alveolar Echinococcosis (AE) FAQs',\n",
       "  \"Hello as your doctors suggested it could be either psoriasis or seborrhea dermatitis. It's difficult to say which one only based on a description. However it is sometimes difficult to distinguish one from the other, but you are on the right track by giving her the correct treatment.it needs to be continued. Regards\"]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63b19b71",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T13:07:10.229856Z",
     "iopub.status.busy": "2025-01-18T13:07:10.229623Z",
     "iopub.status.idle": "2025-01-18T13:07:10.234679Z",
     "shell.execute_reply": "2025-01-18T13:07:10.234007Z"
    },
    "papermill": {
     "duration": 0.01235,
     "end_time": "2025-01-18T13:07:10.235873",
     "exception": false,
     "start_time": "2025-01-18T13:07:10.223523",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': ['What causes Hemorrhagic shock and encephalopathy syndrome ?',\n",
       "  \"What to do for What I need to know about Crohn's Disease ?\"],\n",
       " 'output': ['What causes hemorrhagic shock and encephalopathy syndrome? The cause of hemorrhagic shock and encephalopathy syndrome is unknown. Some researchers believe that this condition is caused by a complex combination of genetic and environmental factors. Researchers have proposed various factors that may contribute to the development of this condition, including infection, exposure to toxins in the environment, and overwrapping of infants with a fever. Hemorrhagic shock and encephalopathy syndrome has not been reported to be associated with a specific ethnic group or religious background.',\n",
       "  \"- Crohn's disease is a disease that causes inflammation, or swelling, and irritation of any part of the digestive tractalso called the gastrointestinal (GI) tract.  - People with Crohns disease may have a blood relative with the disease or another type of inflammatory bowel disease (IBD).  - Symptoms of Crohns disease include abdominal pain, diarrhea, bleeding, weight loss, and fever.  - A physical exam, blood tests, stool tests, and other tests are needed to diagnose Crohns disease.  - Problems of Crohns disease include intestinal blockage, fistulas, abscesses, anemia, and slower growth in children.  - Doctors treat Crohns disease with medicines, surgery, diet, and nutrition.  - People with Crohns disease should eat a healthy diet and avoid foods that make symptoms worse.  - Quitting smoking can help make Crohns disease less severe. Ask your health care provider if you need help quitting smoking.  - Support groups may help lower stress for people with Crohns disease.  - Most people with Crohns disease are able to work, raise families, and live full lives.  - Many women with Crohns disease can become pregnant and have a baby. You should talk with your health care provider before getting pregnant.\"]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"test\"][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40868b6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T13:07:10.247553Z",
     "iopub.status.busy": "2025-01-18T13:07:10.247321Z",
     "iopub.status.idle": "2025-01-18T13:07:10.252535Z",
     "shell.execute_reply": "2025-01-18T13:07:10.251715Z"
    },
    "papermill": {
     "duration": 0.012636,
     "end_time": "2025-01-18T13:07:10.253925",
     "exception": false,
     "start_time": "2025-01-18T13:07:10.241289",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "compute_dtype = torch.bfloat16\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=compute_dtype,\n",
    "        bnb_4bit_use_double_quant=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c97c75a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T13:07:10.265988Z",
     "iopub.status.busy": "2025-01-18T13:07:10.265770Z",
     "iopub.status.idle": "2025-01-18T13:09:53.792242Z",
     "shell.execute_reply": "2025-01-18T13:09:53.791277Z"
    },
    "papermill": {
     "duration": 163.534162,
     "end_time": "2025-01-18T13:09:53.793745",
     "exception": false,
     "start_time": "2025-01-18T13:07:10.259583",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28c22ace94fb4514b59788f5d071a5ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/878 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70ee8f9fb53144468e159c13d5670860",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "553129a325634b2f83a4975fb7a0ffcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1857df6280e54023a3ab4ed0d4b6c01a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37bd0823500f4feea13b6f8b53a975d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92384533afd5487a9615615e6e63699d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "892982530f354280bf74dd571601abbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "122e40de0eba4ee39de73753f0c92794",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63ea98cee56b4eaea3a6f41fad1a38af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffa8901afb5247739f5c23f42b918d9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare model, tokenizer: 163.521 sec.\n"
     ]
    }
   ],
   "source": [
    "time_start = time()\n",
    "\n",
    "model_config = AutoConfig.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    max_new_tokens=1024\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    config=model_config,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto',\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "time_end = time()\n",
    "print(f\"Prepare model, tokenizer: {round(time_end-time_start, 3)} sec.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30338f5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T13:09:53.808554Z",
     "iopub.status.busy": "2025-01-18T13:09:53.808203Z",
     "iopub.status.idle": "2025-01-18T13:10:00.251540Z",
     "shell.execute_reply": "2025-01-18T13:10:00.250594Z"
    },
    "papermill": {
     "duration": 6.452239,
     "end_time": "2025-01-18T13:10:00.253116",
     "exception": false,
     "start_time": "2025-01-18T13:09:53.800877",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    }
   ],
   "source": [
    "tokenizer.chat_template = None\n",
    "model, tokenizer = setup_chat_format(model, tokenizer)\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9cb77ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T13:10:00.267860Z",
     "iopub.status.busy": "2025-01-18T13:10:00.267600Z",
     "iopub.status.idle": "2025-01-18T13:10:00.271981Z",
     "shell.execute_reply": "2025-01-18T13:10:00.271206Z"
    },
    "papermill": {
     "duration": 0.012886,
     "end_time": "2025-01-18T13:10:00.273089",
     "exception": false,
     "start_time": "2025-01-18T13:10:00.260203",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128257"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1e8645f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T13:10:00.286850Z",
     "iopub.status.busy": "2025-01-18T13:10:00.286637Z",
     "iopub.status.idle": "2025-01-18T13:10:00.289581Z",
     "shell.execute_reply": "2025-01-18T13:10:00.288964Z"
    },
    "papermill": {
     "duration": 0.010992,
     "end_time": "2025-01-18T13:10:00.290765",
     "exception": false,
     "start_time": "2025-01-18T13:10:00.279773",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "134f81b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T13:10:00.304988Z",
     "iopub.status.busy": "2025-01-18T13:10:00.304756Z",
     "iopub.status.idle": "2025-01-18T13:10:00.308780Z",
     "shell.execute_reply": "2025-01-18T13:10:00.308158Z"
    },
    "papermill": {
     "duration": 0.012362,
     "end_time": "2025-01-18T13:10:00.309973",
     "exception": false,
     "start_time": "2025-01-18T13:10:00.297611",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def apply_chat_template(example, tokenizer):\n",
    "    inputs = example[\"input\"]\n",
    "    outputs = example[\"output\"]\n",
    "    \n",
    "    # Initialize the messages list with a system message (specific to medical domain)\n",
    "    messages = [{\"role\": \"system\", \"content\": \"You are a medical assistant. Please provide accurate, evidence-based responses to the user's medical questions. If you're unsure about an answer, suggest consulting a healthcare professional.\"}]\n",
    "    \n",
    "    # Add user message\n",
    "    messages.append({\"role\": \"user\", \"content\": inputs})\n",
    "    \n",
    "    # Add assistant message\n",
    "    messages.append({\"role\": \"assistant\", \"content\": outputs})\n",
    "    \n",
    "    # Apply the chat template\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "    \n",
    "    # Return a new dictionary with the modified content\n",
    "    example[\"text\"] = text\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5007791",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T13:10:00.324552Z",
     "iopub.status.busy": "2025-01-18T13:10:00.324309Z",
     "iopub.status.idle": "2025-01-18T13:10:00.627818Z",
     "shell.execute_reply": "2025-01-18T13:10:00.626841Z"
    },
    "papermill": {
     "duration": 0.311905,
     "end_time": "2025-01-18T13:10:00.629062",
     "exception": false,
     "start_time": "2025-01-18T13:10:00.317157",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32353\n",
      "32353\n"
     ]
    }
   ],
   "source": [
    "print(len(raw_datasets['train']['input']))\n",
    "print(len(raw_datasets['train']['output']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8d22d9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T13:10:00.644208Z",
     "iopub.status.busy": "2025-01-18T13:10:00.643955Z",
     "iopub.status.idle": "2025-01-18T13:10:00.718588Z",
     "shell.execute_reply": "2025-01-18T13:10:00.717558Z"
    },
    "papermill": {
     "duration": 0.083558,
     "end_time": "2025-01-18T13:10:00.719927",
     "exception": false,
     "start_time": "2025-01-18T13:10:00.636369",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8089\n",
      "8089\n"
     ]
    }
   ],
   "source": [
    "print(len(raw_datasets['test']['input']))\n",
    "print(len(raw_datasets['test']['output']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5af71446",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T13:10:00.734887Z",
     "iopub.status.busy": "2025-01-18T13:10:00.734621Z",
     "iopub.status.idle": "2025-01-18T13:10:07.799157Z",
     "shell.execute_reply": "2025-01-18T13:10:07.798097Z"
    },
    "papermill": {
     "duration": 7.073493,
     "end_time": "2025-01-18T13:10:07.800680",
     "exception": false,
     "start_time": "2025-01-18T13:10:00.727187",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4de76a25f2a443dca70ca088065e2554",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/32353 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4edfe399f9d44c049ca2427d03fe0833",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8089 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Assuming raw_datasets[\"train\"] is a Dataset object\n",
    "datasets = dict()\n",
    "datasets[\"train\"] = raw_datasets[\"train\"].map(lambda example: apply_chat_template(example, tokenizer))\n",
    "datasets[\"test\"] = raw_datasets[\"test\"].map(lambda example: apply_chat_template(example, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dfed4277",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T13:10:07.816379Z",
     "iopub.status.busy": "2025-01-18T13:10:07.816066Z",
     "iopub.status.idle": "2025-01-18T13:10:07.820743Z",
     "shell.execute_reply": "2025-01-18T13:10:07.820048Z"
    },
    "papermill": {
     "duration": 0.013604,
     "end_time": "2025-01-18T13:10:07.821947",
     "exception": false,
     "start_time": "2025-01-18T13:10:07.808343",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': Dataset({\n",
       "     features: ['input', 'output', 'text'],\n",
       "     num_rows: 32353\n",
       " }),\n",
       " 'test': Dataset({\n",
       "     features: ['input', 'output', 'text'],\n",
       "     num_rows: 8089\n",
       " })}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "127938bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T13:10:07.837413Z",
     "iopub.status.busy": "2025-01-18T13:10:07.837116Z",
     "iopub.status.idle": "2025-01-18T13:10:07.840831Z",
     "shell.execute_reply": "2025-01-18T13:10:07.840010Z"
    },
    "papermill": {
     "duration": 0.012548,
     "end_time": "2025-01-18T13:10:07.841992",
     "exception": false,
     "start_time": "2025-01-18T13:10:07.829444",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set pad_token_id equal to the eos_token_id if not set\n",
    "if tokenizer.pad_token_id is None:\n",
    "  tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Set reasonable default for models without max length\n",
    "if tokenizer.model_max_length > 100_000:\n",
    "  tokenizer.model_max_length = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "77691d60",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T13:10:07.857406Z",
     "iopub.status.busy": "2025-01-18T13:10:07.857120Z",
     "iopub.status.idle": "2025-01-18T13:10:07.863041Z",
     "shell.execute_reply": "2025-01-18T13:10:07.862342Z"
    },
    "papermill": {
     "duration": 0.014932,
     "end_time": "2025-01-18T13:10:07.864165",
     "exception": false,
     "start_time": "2025-01-18T13:10:07.849233",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 13049 of the processed training set:\n",
      "\n",
      "<|im_start|>system\n",
      "You are a medical assistant. Please provide accurate, evidence-based responses to the user's medical questions. If you're unsure about an answer, suggest consulting a healthcare professional.<|im_end|>\n",
      "<|im_start|>user\n",
      "What are the treatments for Unverricht-Lundborg disease ?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "These resources address the diagnosis or management of Unverricht-Lundborg disease:  - Gene Review: Gene Review: Unverricht-Lundborg Disease  - Genetic Testing Registry: Unverricht-Lundborg syndrome   These resources from MedlinePlus offer information about the diagnosis and management of various health conditions:  - Diagnostic Tests  - Drug Therapy  - Surgery and Rehabilitation  - Genetic Counseling   - Palliative Care<|im_end|>\n",
      "\n",
      "#####################################\n",
      "Sample 24316 of the processed training set:\n",
      "\n",
      "<|im_start|>system\n",
      "You are a medical assistant. Please provide accurate, evidence-based responses to the user's medical questions. If you're unsure about an answer, suggest consulting a healthcare professional.<|im_end|>\n",
      "<|im_start|>user\n",
      "What are the treatments for Brown-Sequard syndrome ?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "How might Brown-Sequard syndrome be treated?<|im_end|>\n",
      "\n",
      "#####################################\n"
     ]
    }
   ],
   "source": [
    "# Assuming datasets is already defined as shown in your example\n",
    "train_dataset = datasets[\"train\"]\n",
    "eval_dataset = datasets[\"test\"]\n",
    "\n",
    "# Sample 2 random indices from the training set\n",
    "for index in random.sample(range(len(train_dataset)), 2):\n",
    "    # Print the 'text' field from the processed training set\n",
    "    print(f\"Sample {index} of the processed training set:\\n\\n{train_dataset[index]['text']}\")\n",
    "    print(\"#####################################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f31d70fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T13:10:07.879580Z",
     "iopub.status.busy": "2025-01-18T13:10:07.879323Z",
     "iopub.status.idle": "2025-01-18T13:10:07.908295Z",
     "shell.execute_reply": "2025-01-18T13:10:07.907361Z"
    },
    "papermill": {
     "duration": 0.038415,
     "end_time": "2025-01-18T13:10:07.909945",
     "exception": false,
     "start_time": "2025-01-18T13:10:07.871530",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Path where the Trainer will save its checkpoints and logs\n",
    "trained_model_id = \"Llama-3.2-3B-Instruct-medical-dataset\"\n",
    "output_dir = 'kaggle/working/' + trained_model_id\n",
    "\n",
    "# QLoRA-specific configuration for Llama 3.2 3B Instruct\n",
    "peft_config = LoraConfig(\n",
    "    r=4,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    ")\n",
    "# Training arguments based on config\n",
    "training_args = TrainingArguments(\n",
    "    fp16=False,  # specify bf16=True instead when training on GPUs that support bf16 else fp16\n",
    "    bf16=False,\n",
    "    do_eval=True,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    gradient_accumulation_steps=1,\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    learning_rate=2.0e-05,\n",
    "    log_level=\"info\",\n",
    "    logging_steps=5,\n",
    "    logging_strategy=\"steps\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    max_steps=-1,\n",
    "    num_train_epochs=1,\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    per_device_eval_batch_size=1,  # originally set to 8\n",
    "    per_device_train_batch_size=1,  # originally set to 8\n",
    "    push_to_hub=True,\n",
    "    hub_model_id=f\"{username}/{repository_name}\",\n",
    "    report_to=\"none\",\n",
    "    save_steps=100, \n",
    "    save_strategy=\"no\",\n",
    "    save_total_limit=2,\n",
    "    seed=42,\n",
    "    # QLoRA-specific arguments:\n",
    "    # You might also want to specify `quantization` or related arguments depending on the implementation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0b0498a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T13:10:07.926307Z",
     "iopub.status.busy": "2025-01-18T13:10:07.926038Z",
     "iopub.status.idle": "2025-01-18T13:10:07.931344Z",
     "shell.execute_reply": "2025-01-18T13:10:07.930683Z"
    },
    "papermill": {
     "duration": 0.014683,
     "end_time": "2025-01-18T13:10:07.932597",
     "exception": false,
     "start_time": "2025-01-18T13:10:07.917914",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# del trainer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c3c90662",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T13:10:07.947871Z",
     "iopub.status.busy": "2025-01-18T13:10:07.947657Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2025-01-18T13:10:07.940020",
     "status": "running"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcba4884b3634db5bbed346f6548b9be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/32353 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "813c6d05f8f3471fa8df7ad62ca193c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8089 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Num examples = 32,353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Num Epochs = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Instantaneous batch size per device = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Gradient Accumulation steps = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Total optimization steps = 32,353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Number of trainable parameters = 6,078,464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32354' max='32353' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32353/32353 11:49:50, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='833' max='8089' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 833/8089 05:39 < 49:23, 2.45 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Num examples = 8089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Batch size = 1\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        # dataset_text_field=\"text\",\n",
    "        tokenizer=tokenizer,\n",
    "        # packing=True,\n",
    "        peft_config=peft_config,\n",
    "        # max_seq_length=tokenizer.model_max_length,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3473603",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.push_to_hub()\n",
    "print('Finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e134fa",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example of how to integrate quantization (assuming use of bitsandbytes or similar)\n",
    "from bitsandbytes import load_quantized_model\n",
    "\n",
    "# Load your model with quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    f\"{username}/{repository_name}\",\n",
    "    trust_remote_code=True,\n",
    "    config=model_config,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto',\n",
    ")\n",
    "\n",
    "# Continue with training using the model\n",
    "print(\"Loaded Model\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5886483,
     "sourceId": 9639961,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30822,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-01-18T13:06:03.409560",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}